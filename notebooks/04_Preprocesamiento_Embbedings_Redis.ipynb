{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c82516-9edd-475f-acbc-0ee5c3956a92",
   "metadata": {},
   "source": [
    "# 4. Preparar Documentos\n",
    "En este notebook se desarrolla el preprocesamiento necesario para la generaci√≥n de embeddings a partir de los textos normativos, con el objetivo de indexar la informaci√≥n en una base Redis para su posterior consulta sem√°ntica y recuperaci√≥n eficiente. El proceso abarca la preparaci√≥n de los textos, el uso de modelos de lenguaje para obtener representaciones vectoriales (embeddings) y la estructuraci√≥n de la informaci√≥n para su integraci√≥n en el pipeline de gesti√≥n normativa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d0e1f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import tiktoken\n",
    "import redis as RedisClient\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.redis.base import Redis as RedisVectorStore\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from redis.commands.search.query import Query\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from redis.commands.search.field import TextField, NumericField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb15f3-bb7d-41a0-8f96-6502f78afd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# CONSTANTS\n",
    "# -------------------------------\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_DB = 0\n",
    "REDIS_USERNAME = \"default\"\n",
    "REDIS_PASSWORD = \"\"\n",
    "REDIS_INDEX = \"normativas_sii\"\n",
    "GPT_KEY = \"sk-....\"  # ‚Üê Reempl√°zalo por tu clave real\n",
    "\n",
    "REDIS_URL = f\"redis://{REDIS_USERNAME}:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}\"\n",
    "DATA_DIR =\"../raw_data\"\n",
    "CSV_PATH = \"03_classified_regulations.csv\"\n",
    "MODEL_NAME = \"text-embedding-3-large\"  # Modelo de OpenAI para embeddings\n",
    "# Tokenizador para OpenAI embeddings\n",
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "SAFE_TOKENS = 30000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede98e79",
   "metadata": {},
   "source": [
    "## 4.1 Carga de Datos\n",
    "En esta secci√≥n se realiza la carga de los datos normativos previamente preprocesados. Los datos contienen los textos completos y la metadata relevante asociada a cada normativa. Esta informaci√≥n ser√° utilizada como insumo para la generaci√≥n de los embeddings.\n",
    "\n",
    "Se aplica un preprocesamiento b√°sico a los textos normativos para optimizar la calidad de los embeddings generados. Esto puede incluir normalizaci√≥n de caracteres, eliminaci√≥n de s√≠mbolos innecesarios y, en algunos casos, reducci√≥n de ruido textual. El objetivo es asegurar que la representaci√≥n vectorial capture el contenido sem√°ntico relevante de cada normativa.\n",
    "\n",
    "Utilizando un modelo de lenguaje preentrenado, se obtienen los embeddings para cada texto normativo. Estos vectores num√©ricos permiten comparar y recuperar informaci√≥n normativa de manera eficiente, habilitando funcionalidades avanzadas de b√∫squeda y clasificaci√≥n sem√°ntica.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "685af865-63ad-4253-b64e-ace1f5caae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# FUNCTIONS\n",
    "# -------------------------------\n",
    "def contar_tokens(texto: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a given text using the default tokenizer.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(texto))\n",
    "\n",
    "\n",
    "def resumen_chunks_por_documento(documentos):\n",
    "    \"\"\"\n",
    "    Imprime un resumen del n√∫mero de chunks generados por cada documento.\n",
    "\n",
    "    \"\"\"\n",
    "    conteo = Counter(doc.metadata.get(\"nombre\", \"Sin nombre\") for doc in documentos)\n",
    "    print(f\"Total de chunks generados: {len(documentos)}\")\n",
    "    print(\"Resumen por documento (solo los que tienen m√°s de un chunk):\")\n",
    "    for nombre, cantidad in conteo.items():\n",
    "        if cantidad > 1:\n",
    "            print(f\"- {nombre}: {cantidad} chunk(s)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preparar_documentos(df: pd.DataFrame,\n",
    "                        anios: Union[int, List[int]] = None,\n",
    "                        chunk_size: int = None,\n",
    "                        chunk_overlap: int = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carga y prepara documentos desde un archivo CSV para procesamiento de NLP o indexaci√≥n.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtro por a√±o\n",
    "    if anios:\n",
    "        if isinstance(anios, int):\n",
    "            anios = [anios]\n",
    "        # Convierte a int si tus a√±os est√°n como string\n",
    "        df = df[df[\"anno\"].isin(anios)]\n",
    "\n",
    "\n",
    "    print(f\"üìÑ Cargando {len(df)} documentos desde '{path_csv}' (filtrados por a√±o: {anios})...\")\n",
    "    documentos = []\n",
    "    splitter = None\n",
    "\n",
    "    if chunk_size and chunk_overlap:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        metadatos = {\n",
    "            \"nombre\": row.get(\"nombre\", \"\"),\n",
    "            \"descripcion\": row.get(\"descripcion\", \"\"),\n",
    "            \"fuente\": row.get(\"fuente\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"tipo_documento\": row.get(\"tipo_documento\", \"\"),\n",
    "            \"relevancia\": row.get(\"relevancia\", \"\"),\n",
    "            \"explicacion\": row.get(\"explicacion\", \"\"),\n",
    "            \"anno\": safe_int(row.get(\"anno\", \"\")),\n",
    "            \"n_tokens\": safe_int(contar_tokens(str(row[\"corpus\"]).strip())),\n",
    "            \"exitos\": safe_int(row.get(\"exitos\", \"\")),\n",
    "            \"peso\": safe_int(row.get(\"peso\", \"\")),\n",
    "          \n",
    "        }\n",
    "        cuerpo = str(row[\"corpus\"]).strip()\n",
    "        total_tokens = contar_tokens(cuerpo)\n",
    "        print(f\"üîç Procesando documento: {row.get('nombre', '(sin nombre)')} - Tokens: {total_tokens}\")\n",
    "        if total_tokens <= SAFE_TOKENS:\n",
    "            documentos.append(Document(page_content=cuerpo, metadata=metadatos))\n",
    "        elif splitter:\n",
    "            partes = splitter.split_text(cuerpo)\n",
    "            for parte in partes:\n",
    "                parte_limpia = parte.strip()\n",
    "                if contar_tokens(parte_limpia) <= SAFE_TOKENS:\n",
    "                    documentos.append(Document(page_content=parte_limpia, metadata=metadatos))\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Chunk excede el m√°ximo seguro de tokens, se omiti√≥ parcialmente.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Documento omitido por superar l√≠mite de tokens y no hay splitter definido.\")\n",
    "\n",
    "    return documentos\n",
    "\n",
    "\n",
    "\n",
    "def buscar_knn_normativas(query: str, k: int = 5, min_score: float = 0.0):\n",
    "    \"\"\"\n",
    "    Realiza una b√∫squeda KNN en Redis (con m√©trica COSINE) para encontrar normativas similares a una consulta.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener embedding\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=MODEL_NAME,  # Dimensi√≥n fija de 6144\n",
    "        openai_api_key=GPT_KEY\n",
    "    )\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "\n",
    "    # Conexi√≥n Redis\n",
    "    r = RedisClient.Redis(\n",
    "        host=REDIS_HOST,\n",
    "        port=int(REDIS_PORT),\n",
    "        db=REDIS_DB,\n",
    "        username=REDIS_USERNAME,\n",
    "        password=REDIS_PASSWORD,\n",
    "        decode_responses=True\n",
    "    )\n",
    "\n",
    "    # B√∫squeda KNN\n",
    "    base_query = f\"*=>[KNN {k} @content_vector $vec as vector_score]\"\n",
    "    q = (\n",
    "        Query(base_query)\n",
    "        .return_fields(\"nombre\",\"descripcion\",\"relevancia\", \"explicacion\", \"url\", \"peso\",\"exitos\",\"vector_score\")\n",
    "        .sort_by(\"vector_score\")  # Ordena por menor distancia\n",
    "        .dialect(2)\n",
    "    )\n",
    "\n",
    "    results = r.ft(\"normativas_sii\").search(\n",
    "        q, query_params={\"vec\": np.array(query_vector, dtype=np.float32).tobytes()}\n",
    "    )\n",
    "    # Procesar resultados\n",
    "    filas = []\n",
    "    for doc in results.docs:\n",
    "        distancia = float(doc.vector_score)\n",
    "        similitud = 1 - distancia  # convertir a similitud 0-1\n",
    "        if similitud >= min_score:\n",
    "            nombre = getattr(doc, \"nombre\", \"\")\n",
    "            url = getattr(doc, \"url\", \"\")\n",
    "            nombre_link = f'<a href=\"{url}\" target=\"_blank\">{nombre}</a>' if url else nombre\n",
    "\n",
    "            filas.append({\n",
    "                \"nombre\": nombre_link,\n",
    "                \"descripcion\": getattr(doc, \"descripcion\", \"\"),\n",
    "                \"peso\": getattr(doc, \"peso\", \"\"),\n",
    "                \"exitos\": getattr(doc, \"exitos\", \"\"),\n",
    "                \"relevancia\": getattr(doc, \"relevancia\", \"\"),\n",
    "                \"explicacion\": getattr(doc, \"explicacion\", \"\"),\n",
    "                \"vector_score\": round(similitud, 4)  # ahora es similitud\n",
    "            })\n",
    "\n",
    "    # Mostrar en HTML\n",
    "    df = pd.DataFrame(filas)\n",
    "    display(HTML(df.to_html(escape=False, index=False)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_int(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2a7774ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def delete_index(r):\n",
    "    if not eliminar_anteriores:\n",
    "        return\n",
    "    print(\"üßπ Eliminando √≠ndice anterior...\")\n",
    "    try:\n",
    "        r.ft(redis_index).dropindex(delete_documents=True)\n",
    "        print(f\"‚úÖ √çndice '{redis_index}' eliminado correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è No se pudo eliminar √≠ndice previo: {e}\")\n",
    "\n",
    "def indexar_en_redis_optimizado(documentos, redis_url, redis_index, gpt_key,\n",
    "                               eliminar_anteriores=False,\n",
    "                               modelo_embeddings=\"text-embedding-3-small\",\n",
    "                               batch_size=50, max_workers=5):\n",
    "    \"\"\"\n",
    "    Indexa documentos en Redis optimizando:\n",
    "    - Dimensiones (6144 para large, 1536 para small)\n",
    "    - Procesamiento en paralelo\n",
    "    - Manejo por lotes\n",
    "    \"\"\"\n",
    "\n",
    "    r = RedisClient.Redis.from_url(redis_url)\n",
    "    delete_index(r)\n",
    "    \n",
    "    # üîπ Configurar dimensiones seg√∫n el modelo\n",
    "    DIM = 6144 if \"large\" in modelo_embeddings else 1536\n",
    "\n",
    "    # üîπ Inicializar embeddings\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=modelo_embeddings,\n",
    "        openai_api_key=gpt_key,\n",
    "    )\n",
    "\n",
    "\n",
    "    # üîπ Crear nuevo √≠ndice\n",
    "    schema = [\n",
    "        TextField(\"nombre\"),\n",
    "        TextField(\"descripcion\"),\n",
    "        TextField(\"fuente\"),\n",
    "        TextField(\"url\"),\n",
    "        TextField(\"tipo_documento\"),\n",
    "        TextField(\"relevancia\"),\n",
    "        TextField(\"explicacion\"),\n",
    "        NumericField(\"anno\"),\n",
    "        NumericField(\"n_tokens\"),\n",
    "        NumericField(\"peso\"),\n",
    "        NumericField(\"exitos\"),\n",
    "        VectorField(\"content_vector\", \"HNSW\", {\n",
    "            \"TYPE\": \"FLOAT32\",\n",
    "            \"DIM\": DIM,\n",
    "            \"DISTANCE_METRIC\": \"COSINE\"\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        r.ft(redis_index).create_index(\n",
    "            fields=schema,\n",
    "            definition=IndexDefinition(prefix=[\"doc:\"], index_type=IndexType.HASH)\n",
    "        )\n",
    "        print(f\"‚úÖ √çndice '{redis_index}' creado con COSINE y DIM={DIM}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è Posible √≠ndice ya existente: {e}\")\n",
    "\n",
    "    errores = []\n",
    "    total_indexados = 0\n",
    "\n",
    "    # üîπ Funci√≥n para procesar lote\n",
    "    def procesar_lote(lote):\n",
    "        try:\n",
    "            RedisVectorStore.from_documents(\n",
    "                documents=lote,\n",
    "                embedding=embeddings,\n",
    "                redis_url=redis_url,\n",
    "                index_name=redis_index\n",
    "            )\n",
    "            return len(lote), []\n",
    "        except Exception as e:\n",
    "            return 0, [{\"error\": str(e), \"documentos\": [d.metadata.get('nombre') for d in lote]}]\n",
    "\n",
    "    # üîπ Procesar en lotes y paralelo\n",
    "    print(f\"üöÄ Iniciando indexaci√≥n de {len(documentos)} documentos en lotes de {batch_size}...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for i in range(0, len(documentos), batch_size):\n",
    "            lote = documentos[i:i+batch_size]\n",
    "            futures.append(executor.submit(procesar_lote, lote))\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"üì¶ Indexando\"):\n",
    "            count, errs = future.result()\n",
    "            total_indexados += count\n",
    "            errores.extend(errs)\n",
    "\n",
    "    print(f\"‚úÖ Total indexados: {total_indexados}/{len(documentos)}\")\n",
    "\n",
    "    if errores:\n",
    "        df_err = pd.DataFrame(errores)\n",
    "        df_err.to_csv(\"errores_indexacion.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(\"‚ö†Ô∏è Errores registrados en 'errores_indexacion.csv'\")\n",
    "\n",
    "    return total_indexados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771483c",
   "metadata": {},
   "source": [
    "## 4.2 Preprocesar Documentos\n",
    "Se estructura la informaci√≥n para su integraci√≥n en Redis, asignando claves √∫nicas a cada normativa y asociando sus respectivos embeddings. Esta organizaci√≥n es fundamental para facilitar la recuperaci√≥n r√°pida y precisa de normativas relevantes mediante consultas vectoriales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbade2eb-293b-42d9-9ea0-a0da2d04050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar muestra de documentos cargados\n",
    "path_csv=f\"{DATA_DIR}/{CSV_PATH}\"\n",
    "df = pd.read_csv(path_csv)\n",
    "\n",
    "\n",
    "documentos = preparar_documentos(\n",
    "    df=df,\n",
    "    anios=[2020,2021,2022,2023,2024],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e8b77646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Se procesaran los documentos de los a√±os [2020, 2021, 2022, 2023, 2024\n",
      "Total de chunks generados: 2910\n",
      "Resumen por documento (solo los que tienen m√°s de un chunk):\n",
      "- Circular N¬∞ 31 del 19 de Mayo del 2021: 160 chunk(s)\n",
      "- Circular N¬∞ 62 del 24 de Septiembre del 2020: 358 chunk(s)\n",
      "- Circular N¬∞ 73 del 22 de Diciembre del 2020: 503 chunk(s)\n",
      "- Circular N¬∞ 43 del 05 de Julio del 2021: 175 chunk(s)\n",
      "- Circular N¬∞ 41 del 02 de Julio del 2021: 181 chunk(s)\n",
      "- Circular N¬∞ 12 del 17 de Febrero del 2021: 274 chunk(s)\n",
      "- Circular N¬∞ 53 del 10 de Agosto del 2020: 234 chunk(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìÑ Se procesaran los documentos de los a√±os [2020, 2021, 2022, 2023, 2024\")\n",
    "resumen_chunks_por_documento(documentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f68f0-38a7-4ba0-bf2c-75e908d0c78f",
   "metadata": {},
   "source": [
    "# 4.3 Preparaci√≥n para indexaci√≥n en Redis\n",
    "Se estructura la informaci√≥n para su integraci√≥n en Redis, asignando claves √∫nicas a cada normativa y asociando sus respectivos embeddings. Esta organizaci√≥n es fundamental para facilitar la recuperaci√≥n r√°pida y precisa de normativas relevantes mediante consultas vectoriales.\n",
    "\n",
    "Se realiza la conexi√≥n con la instancia de Redis y se procede a la carga de los embeddings generados junto con la metadata correspondiente. Este paso habilita la infraestructura para consultas sem√°nticas, permitiendo la b√∫squeda y recomendaci√≥n de normativas en funci√≥n de la similitud entre textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f651bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc1ee9-6fbf-4888-a5b2-f7f846f6b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexar_en_redis(documentos, REDIS_URL, REDIS_INDEX, GPT_KEY, eliminar_anteriores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0876c6b",
   "metadata": {},
   "source": [
    "## 4.4 Validaci√≥n del proceso\n",
    "Se efect√∫an pruebas de validaci√≥n sobre la carga y consulta de embeddings en Redis para asegurar la correcta integraci√≥n del flujo. Se verifican tanto la recuperaci√≥n de los vectores como la consistencia de la metadata asociada a cada normativa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a3fd97ed-18dc-4dd6-8ce8-31dda0a0c45d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>nombre</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>peso</th>\n",
       "      <th>exitos</th>\n",
       "      <th>relevancia</th>\n",
       "      <th>explicacion</th>\n",
       "      <th>vector_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><a href=\"https://www.sii.cl/normativa_legislacion/resoluciones/2022/reso53.pdf\" target=\"_blank\">Resoluci√≥n Exenta SII N¬∞ 53 del 09 de Junio del 2022</a></td>\n",
       "      <td>Modifica resoluci√≥n Ex. SII N¬∞ 74 de fecha 02 de julio de 2020, eliminando obligaci√≥n de enviar el resumen de ventas diarias en la forma y condiciones que indica.</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>Contiene 'cumplimiento tributario' | Contiene 'boletas electr√≥nicas'</td>\n",
       "      <td>0.6456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><a href=\"https://www.sii.cl/normativa_legislacion/resoluciones/2020/reso104.pdf\" target=\"_blank\">Resoluci√≥n Exenta SII N¬∞ 104 del 02 de Septiembre del 2020</a></td>\n",
       "      <td>Modifica fecha de entrada en vigencia de la resoluci√≥n Ex. SII N¬∞ 74 de fecha 02 de julio de 2020, que instruye procedimiento para emitir boletas electr√≥nicas y boletas no afectas y exentas electr√≥nicas de ventas y servicios.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>Contiene 'boletas electr√≥nicas'</td>\n",
       "      <td>0.6381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = buscar_knn_normativas(\"¬øQu√© normativas boletas electr√≥nicas?\", k=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1d322458-fede-4f6b-ab52-6ccc3b010ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>nombre</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>peso</th>\n",
       "      <th>exitos</th>\n",
       "      <th>relevancia</th>\n",
       "      <th>explicacion</th>\n",
       "      <th>vector_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td><a href=\"https://www.sii.cl/normativa_legislacion/resoluciones/2023/reso106.pdf\" target=\"_blank\">Resoluci√≥n Exenta SII N¬∞ 106 del 06 de Septiembre del 2023</a></td>\n",
       "      <td>Aprueba Convenio Interadministrativo entre la Tesorer√≠a General de la Rep√∫blica, el Banco del Estado de Chile y el Servicio de Impuestos Internos para disponibilizar canales de pago que faciliten la recaudaci√≥n y el pago de tributos y otros ingresos p√∫blicos.</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Relevante</td>\n",
       "      <td>Contiene 'pagos electr√≥nicos' | Contiene 'cumplimiento tributario'</td>\n",
       "      <td>0.3937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td><a href=\"https://www.sii.cl/normativa_legislacion/resoluciones/2020/reso91.pdf\" target=\"_blank\">Resoluci√≥n Exenta SII N¬∞ 91 del 21 de Agosto del 2020</a></td>\n",
       "      <td>Establece la obligaci√≥n de informar las transacciones con tarjetas de pago seg√∫n lo que se indica.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Relevante</td>\n",
       "      <td>No se encontraron reglas de negocio</td>\n",
       "      <td>0.3777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = buscar_knn_normativas(\"Encontrar medios de pago\", k=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de16296",
   "metadata": {},
   "source": [
    "## 4.5 Conclusiones\n",
    "\n",
    "En esta etapa se desarroll√≥ el preprocesamiento y la preparaci√≥n de los textos normativos para la generaci√≥n de embeddings vectoriales, con el prop√≥sito de habilitar la indexaci√≥n y recuperaci√≥n sem√°ntica eficiente en una base de datos Redis. El flujo de trabajo implementado incluy√≥ la carga de datos preprocesados, la normalizaci√≥n y depuraci√≥n adicional de los textos, y la obtenci√≥n de representaciones vectoriales mediante modelos de lenguaje avanzados.\n",
    "\n",
    "Posteriormente, los documentos y sus embeddings fueron organizados y estructurados para su integraci√≥n en Redis, asignando claves √∫nicas y asociando la metadata relevante. Se estableci√≥ un pipeline robusto para la indexaci√≥n, que contempla la gesti√≥n de l√≠mites de tokens, la fragmentaci√≥n inteligente de textos largos (chunking), y el control de errores en la carga masiva, asegurando la integridad y trazabilidad de los datos.\n",
    "\n",
    "Este proceso permite habilitar funcionalidades de b√∫squeda y recuperaci√≥n sem√°ntica avanzada, donde consultas en lenguaje natural pueden ser eficientemente mapeadas y comparadas contra el corpus normativo. La infraestructura construida, basada en Redis y modelos de embeddings de √∫ltima generaci√≥n, sienta las bases para el desarrollo de sistemas de recomendaci√≥n, an√°lisis automatizado y consulta inteligente de normativas legales, contribuyendo a la modernizaci√≥n y eficiencia en la gesti√≥n documental y normativa.\n",
    "\n",
    "La metodolog√≠a implementada garantiza la reproducibilidad, escalabilidad y calidad del sistema, posicionando la soluci√≥n para aplicaciones futuras en an√°lisis jur√≠dico, cumplimiento normativo asistido por IA y automatizaci√≥n avanzada de flujos legales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb_capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
